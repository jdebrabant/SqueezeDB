\subsection{VC-Dimension and SQL Queries}\label{sec:vcdimquer}
\citet{RiondatoACZU11} defined a range space for SQL queries and computed a
bound to its VC-Dimension. We recall here some of their results that we will use
throughout the paper.

Let $Q$ be a collection of (non-group-by, non-aggregate) queries. We define a
range space $S_Q=(X,\mathcal{R})$ associated to $Q$ as follows. The domain $X$
is the \emph{Cartesian product} of the tables involved in queries of $Q$. For
each query $q\in Q$, let $R_q$ be the subset of $X$ in the output of $q$. The
set $\mathcal{R}$ is the collection of all the ranges $R_q$ for each $q\in Q$.
In the rest of the paper we will often identify a query $q$ with its range $R_q$
(i.e., with its output) and the set of range $\mathcal{R}$ with the class of
queries $Q$. When the ranges represent all the possible outputs of queries in a class $Q$
applied to database tables $\Db$, the VC-dimension of the range space is the maximum
number of tuples such that any subset of them is the output of a query in $Q$.
The following lemmas show how to bound the VC-dimension of the
range space of different classes of queries.

\begin{lemma}[\citep{RiondatoACZU11}]\label{lem:vcdimselgen}
  Let $\Tab$ be a table with $m$ columns, let $b>0$ and let $\Sigma^{b*}_\Tab$
  be the set of selection queries on $\Tab$ whose selection predicate is a
  Boolean combination of up to $b$ clauses. Then, the VC-dimension of the range
  space $S_b = (\Tab, \Sigma^{b*}_\Tab)$ is at most $3((m+1)b)\log((m+1)b)$.  
\end{lemma}

\begin{lemma}[\citep{RiondatoACZU11}]\label{lem:vcdimjoinmul}
  Consider the class $Q$ of queries that can be seen as combinations of select
  and joins on $u>2$ tables $\Tab_1,\dotsc,\Tab_u$. Let $S_i=(\Tab_i,R_i)$,
  $i=1,\dotsc,u$ be the range space associated with the select queries on the $u$
  tables. Let $v_i=VC(S_i)$. Let $m$ be the maximum number of columns in a table
  $\Tab_i$. We assume $m\le \sum_i v_i$.\footnote{The assumption $m\le \sum_i
  v_i$ is reasonable for any practical case.} Let $S_Q = (\Tab_1\times\dotsb\times
  \Tab_u, R_Q)$ be the range space associated with the class $Q$. The range set
  $R_Q$ is defined as follows. Let $\rho = (r_1,\dotsc,r_u)$, $r_i\in R_i$, and
  let $\omega$ be a sequence of
  $u-1$ join conditions representing a possible way to join the $u$ tables $\Tab_i$,
  using the operators $\{>,<,\ge,\le,=,\neq\}$. We define the range 
  \[
  J^\omega_{\rho} = \{(t_1,\dotsc,t_u) ~:~ t_i\in r_i, \text{ s.t. }
  (t_1,\dotsc,t_u) \text{ satisfies } \omega\}.\]
  $R_Q$ is the set of all possible $J^\omega_{\rho}$. Then,
  \[
  VC(S_Q)\leq 4u(\sum_i VC(S_i))\log(u\sum_i VC(S_i)).
  \]
\end{lemma}

\note{We can probably remove the above two lemmas and just state the following.}
The following Theorem can be obtained by combining the above results.

\begin{theorem}\label{thm:vcdimgenqueries}
Let $b$ be a positive integer parameter. Given a set
$\Db=\{\Tab_1,\dotsc,\Tab_u\}$ of $u$ tables and a set $\mathcal{C}_i$ of
up to $m$ columns for each table $T_i$, consider the class $Q_{u,m,b}$ of all
queries on $\Db$ with up to $u-1$ join and $u$ select operations, where the
part of the selection predicate involving table $\Tab_i$ has conditions about
some of the columns from $\mathcal{C}_i$ and up to $b$ Boolean operations, then 
\[
\VC((\Db,Q_{u,m,b})) \leq 
12u^2(m+1)b\log((m+1)b)\log(3u^2(m+1)b\log((m+1)b)).\]
\end{theorem}
It is clear from this Theorem that the bound to the VC-dimension depends
only on the number of tables $u$, the number of columns $m$, and the number of
joins $b$. We can then define the function
\[
\vc(u,m,b)=12u^2(m+1)b\log((m+1)b)\log(3u^2(m+1)b\log((m+1)b)).\]

\citet{RiondatoACZU11} used Thm.~\ref{thm:eapprox} and the above bound to the
VC-dimension of the range space $S=(X,Q)$ for a collection of queries $Q$ to
compute a sample size $m$ such that a sample $\Sam$ of size $m$ of the database
$\Db$ can be used to compute high-quality approximations of the selectivities on
$\Db$ of all queries in $Q$. More precisely, the selectivity
$\selectivity_\Sam(q)$ of $q\in Q$ on $\Sam$ can be used as a good estimation
for $\selectivity_\Db(q)$. Formally, \citet{RiondatoACZU11} proved that it is
possible to build a sample $\Sam$ such that
\[
\Pr\left(\exists q\in Q \text{ s.t.~}
|\selectivity_\Db(q)-\selectivity_\Sam(q)|>\varepsilon\right)\le\delta,
\]
for some $\varepsilon$ and $\delta$ in $(0,1)$.

\todo{Say something about how to create the sample.
Reference~\citep{RiondatoACZU11}.}

In this present work, we leverage on this result to compute good approximations
(and confidence bounds) of the values of aggregate queries using a random sample
of $\Db$.

\subsection{Estimating the Selectivity of \texttt{GROUP BY} Queries}\label{sec:groupby}
To warm up, we show how to use the results introduced above to compute the
selectivity of group-by queries.

Let $\Db={\Tab_1,\dots,\Tab_u}$, and $\mathcal{C_i}$ ($1\le i \le u$), $m$, $b$,
and $Q_{u,m,b}$ be as in Thm.~\ref{thm:vcdimgenqueries}. For a positive integer
$k$, let $Q^k_{u,m,b}$ be the class of group-by queries $q=(q^*,\mathcal{G})$
where $q^*\in Q_{u,m,b}$ and $\mathcal{G}=\{G_1,\dots,G_\ell\}$ is a subset of
the columns in the tables of $\Db$, under the condition that no more than $k$
columns \emph{per table} are in $\mathcal{G}$. We use the equivalence from
Fact~\ref{fact:groupbyequiv} to show how to compute an approximation of the
sizes of the groups in the output of a group-by query. 

Given $\mathcal{G}$, for any query $q\in Q^k_{u,m,b}$, let $E_{q,\mathcal{G}}$
be the set of queries with selection/join predicate as
in~\eqref{eq:groupbyequiv}. It is easy to see that all $q'\in E_{q,\mathcal{G}}$
are members of the class $Q_{u,p,b+2k}$, where $p=\max\{|C_i\cap\mathcal{G}|
~:~ 1\le i\le u\}$ ($p\le m+k$. The class $Q_{u,p,b+2k}$ contains also other
queries that are not members of any $E_q$). 
\begin{fact}
From Thm.~\ref{thm:vcdimgenqueries}, we have that $\VC((\Db,Q_{u,p,b+2k}))\le\vc(u,p,b+2k)$.
\end{fact}

Hence, if we build an $\varepsilon$-approximation $\Sam$ for the range space
$(\Db,Q_{u,p,b+2k})$ (for example using Theorem~\ref{thm:eapprox}) and execute
queries from $Q^k_{u,m,b}$ on $\Sam$, then the selectivities on $\Sam$ of queries
from $E_q$ will be within $\varepsilon$ from their real selectivities. Thanks to
the equivalence from Fact~\ref{fact:groupbyequiv}, this means that we can
estimate the sizes of the groups in the output of a query $q\in Q^k_{u,m,b}$ by
looking at the sizes of the groups in the output of $q$ when run on $\Sam$.

\question{Should the above be more formal and end with a lemma?}

Note that our guarantees do not extend to the number of groups, i.e., we cannot
guarantee that the number of groups in the output of the query when run on the
sample is within $\varepsilon$ from the number of groups in the output of the
query when run on the large database.

\section{Approximately Answering Aggregate Queries}\label{sec:aggreg}
The focus of this work is computing approximate answers to aggregate database
queries by running them on a random sample of the original database. In this
section we first describe our method to compute the \emph{point estimation}
(i.e., the estimation of the value of the query) and then show how to derive
deterministic confidence intervals for our estimations by solving appropriate
optimization problems defined on a polytope.
\todo{State clearly and formally our goals:``compute good approximation and deterministic
confidence intervals for all queries in a certain class, etc etc''}

\subsection{Point Estimation}\label{sec:pointest}
Given a database $\Db$, let $q=(q^*,f)$ be an aggregate query with
$f:\mathcal{P}(\domain{C_f}\times\mathbb{N}_0)\to\mathbb{R}$ for some column $C_f$ of some
table of $\Db$ (we used Fact~\ref{fact:aggregquery} to define $f$). Let $S$ be the
set of pairs $(v,c_v)$ where $v\in \domain{C_f}$ and $c_v$ is the number of times
that $v$ appears in the column $C_f$ in the output of $q^*$ when $q^*$ is run on
$\Db$. Let $\mathcal{C}_{q^*}$ be the selection/join predicate of $q^*$. We can
see the output of $q^*$ as the union of the outputs of the queries whose
selection/join predicate is $\mathcal{C}_{q^*}$
\texttt{AND}'ed with a condition on the column $C_f$, i.e., whose selection/join
predicate is in the form
\begin{equation}\label{eq:aggregselpred}
  \mathcal{C}_{q^*} \AND C_f=v
\end{equation}
where $v\in\domain{C_f}$. Formally, let $q^*_v$ be a query with selection
predicate as in~\eqref{eq:aggregselpred}, for $v\in\domain{C_f}$, and let
$B_{q^*,f}=\{q^*_v ~:~ v\in\domain{C_f}\}$. Then, for any database $\Db$, the
output $\Out_\Db(q^*)$ of $q^*$ on $\Db$ is the union of the outputs of all
queries in $B_{q^*,f}$:
\[ \Out_\Db(q^*)=\bigcup_{v\in\domain{C_f}}\Out_\Db(q^*_v)\enspace.
\]
Given a sample $\Sam$ of $\Db$, we approximate $f(S)$, i.e., the output value of $q$,
with $\tilde f(\tilde S)$ where $\tilde S$ is the collection of pairs
$(v,\card_\Sam(q^*_v))$ for $v\in\domain{C_f}$ and $\tilde f$ is a function from
$\mathcal{P}(\domain{C_f}\times\mathbb{N}_0)$ to $\mathbb{R}$ that can be
specified by the user and describe a procedure to compute an estimator for
$f(S)$. In some cases, for example for $f=\text{\texttt{AVG}}$ we may have
$f=\tilde f$ but in other cases this may not
be true. For example when $f=\text{\texttt{SUM}}$ we need to ``scale up'' the
values $\card_\Sam(q^*_v)$ by multiplying each of them by the ratio between the
product of the sizes of the input tables to $q^*$ and the product of the sizes
of the corresponding sampled tables. Other cases for having $\tilde f\neq f$
include $f=\text{\texttt{VAR}}$ or $f=\text{\texttt{STDEV}}$: if an unbiased
and/or consistent estimator for $f(S)$ is desired, appropriate statistical
procedures exist to achieve this goal using a specific function $\tilde f$.
\question{Make an example?}
Note that we are not yet assuming anything about $\Sam$ or requiring that it has
any specific property: it can be any sample of the database $\Db$. In the next
section instead, we will require that $\Sam$ is a $\varepsilon$-approximation of
an appropriate range space so that we can compute deterministic confidence
intervals for the point estimation.

\subsection{Deterministic confidence intervals}\label{sec:confint}
Single point estimations for a parameter $\theta$ of interest, although useful on
their own, are much more informative when accompanied with \emph{confidence
intervals}. These are intervals of the domain of $\theta$ and give information
about the accuracy of the estimation (which falls within the interval) and the
location of the real value of $\theta$. Usually, confidence bounds are
\emph{probabilistic}, in the following sense. Given a quantity $\theta$ to be
estimated, a $1-\alpha$ confidence interval $I$ is an interval of the domain of
$\theta$ such that $I$ contains $\theta$ with probability $1-\alpha$. The
probability $1-\alpha$ should be interpreted in a frequentist framework. It
represents the fraction of times that the interval (computed according to a fix
procedure) contains the real value of quantity $\theta$, over an infinite number
of repetitions of the procedure to compute the
interval. In our settings and given our goals \itodo{which we never formally
specified\ldots}, deriving probabilistic confidence bounds is not a viable
option. 
\todo{Here we should explain that we cannot use these because we would
have to apply a union bound over a very high number of queries, defeating the
purpose.} 
Our approach instead guarantees that, if the sample $\Sam$ is an
$\varepsilon$-approximation for the range space $(\Db,Q)$ (where $Q$ is the
class of queries that one may want to run), then the confidence interval we
compute contains the real value of the query with probability $1$, for all
aggregate queries built in $Q$. Another advantage of our method compared to
probabilistic confidence intervals is that deriving a procedure to compute the
latter is highly-specific on the aggregate function to be estimated (e.g.,
average rather than standard deviation), while our method is valid for
\emph{any} aggregate function. \citet{Haas96} developed deterministic confidence
intervals for aggregate queries, but our method computes intervals that are
\emph{uniformly more powerful} than those thanks to the fact that we can
leverage the properties of
$\varepsilon$-approximations.

\todo{\citep{Haas96} has some tricks that may help us getting better
estimations. And also he says something for DISTINCT elements. Check those.}

\paragraph{The method.} Our method for computing deterministic confidence
intervals exploits the properties of the $\varepsilon$-approximation. 
In particular, we use the fact that the selectivity on $\Sam$ of all the queries in an
appropriately defined class of queries, is close (within $\varepsilon$) from
their real selectivity on $\Db$.

Let $Q_{u,m,b}$ be as in Thm.~\ref{thm:vcdimgenqueries}, and let
$\mathcal{A}=Q_{u,m,b}\times\mathcal{F}$ \iquestion{$\leftarrow$ Is this
the right expression?} be a class of aggregate queries $(q^*,f)$
where $q^*\in Q_{u,m,b}$ and $f\in\mathcal{F}$, for some family $\mathcal{F}$ of
functions such that each $f\in\mathcal{F}$ goes from
$\mathcal{P}(\domain{\Tab.C}\times\mathbb{N}_0)$ to $\mathbb{R}$, where $\Tab.C$
is a column in some table $\Tab$ of $\Db$. Let $q^*\in Q_{u,m,b}$. As we said in
Sect.~\ref{sec:pointest}, we can see the output of $q^*$ as the union of the
outputs of the queries in $B_{q^*,f}$, whose predicate is as
in~\eqref{eq:aggregselpred}. 

Let now $\mathsf{A}(Q_{u,m,b},\mathcal{F})$ be the union of all $B_{q^*,f}$, for
all $q^*\in Q_{u,m,b}$ and $f\in\mathcal{F}$. Consider the class
$\bar{Q}=Q_{u,m,b}\cup \mathsf{A}(Q_{u,m,b},\mathcal{F})$. Clearly $\bar{Q}$ is
a subset of the class $Q_{u,m+1,b+1}$. 

\begin{fact}
From Thm.~\ref{thm:vcdimgenqueries} we have that the range space
$(\Db,Q_{u,m+1,b+1})$ has VC-dimension at most $\vc(u,m+1,b+1)$. 
\end{fact}

If we build an $\varepsilon$-approximation $\Sam$ for
the range space $(\Db,Q_{u,m+1,b+1})$, we have that the selectivity
$\selectivity_\Sam(Q)$ on $\Sam$ of each query $q\in Q_{u,m+1,b+1})$  will be
within $\varepsilon$ from its real selectivity $\selectivity_\Db(q)$ on $\Db$.
If we fix $\varepsilon,\delta\in(0,1)$ and use Thm.~\ref{thm:eapprox} to
compute the sample $\Sam$, we have that 
\[
\Pr(\exists q\in Q_{u,m+1,b+1} \text{ s.t. }
|\selectivity_\Db(q)-\selectivity_\Sam(q)|>\varepsilon)\le\delta\enspace.
\]
From now on we will assume that the sample $\Sam$ is a
$\varepsilon$-approximation for $(\Db,Q_{u,m+1,b+1})$. \citet{RiondatoACZU11}
describe how to build $\Sam$.

Let $V_{q^*}(C_f)$ be the set of values from $\domain{C_f}$ that appear in the
column $C_f$ in the output of $q^*$ when run on $\Db$ (each value only appears
once:  $V_{q^*}(C_f)$ is \emph{not} a multi-set). For now, assume that we
know $V_{q^*}(C_f)$. We will remove this assumption later. Since we assume we
can build a total order relationship on $\domain{C_f}$, let $v_1,\dotsc,v_\ell$
be a labelling of the values in $V_{q^*}(C_f)$ in increasing sorted order, with
$\ell=|V_{q^*}(C_f)$. Let
$q=(q^*,f)$, and let $\mathcal{C}$ be the selection/join predicate of $q^*$. Let
$a,b$ be two values in $[1,\ell]$. W.l.o.g.~assume $a\le b$. Consider the
query $q^*_{a,b}$ whose selection predicate is
\begin{equation}\label{eq:aggregselpredab}
\mathcal{C} \AND C_f \ge v_a \AND C_f \le v_b\enspace.
\end{equation}
Clearly $q^*_{v_i}=q^*_{i,i}$ for all $v_i\in V_{q^*}(C_f)$. Let now
$\varepsilon_{v_i}=\selectivity_\Db(q^*_{v_i})-\selectivity_\Sam(q^*_{v_i})$. Since we
do not know $\selectivity_\Db(q^*_{v_i})$, we do not know $\varepsilon_{v_i}$ either.
Nevertheless, given that $\Sam$ is a $\varepsilon$-approximation for
$(\Db,Q_{u,m+1,b+1})$, we have that
$\varepsilon_{v_i}\in[-\varepsilon,\varepsilon]$. Consider now a query $q^*_{a,b}$.
It should be clear that its output on any database is the union of the outputs
of the queries $q^*_{v_i}$, for $i\in[a,b]$. Then, we have that
\[
\selectivity_\Db(q^*_{a,b})-\selectivity_\Sam(q^*_{a,b})=\sum_{i=a}^b\varepsilon_{v_i}.
\]
At a first look, it may seem possible that this deviation is quite large, but
actually it is not so and it is bounded by $\pm\varepsilon$. Too see this, it is
sufficient to realize that the query $q^*_{a,b}$, whose selection/join predicate
is as in~\eqref{eq:aggregselpredab}, also belongs to $Q_{u,m+1,b+1}$.
\todo{Check the $b+1$. It's actually correct, but the notation is weird because
of the way we defined the meaning of $b$.}
Given that $\Sam$ is a $\varepsilon$-approximation for $(\Db,Q_{u,m+1,b+1})$, we
have that 
\begin{equation}\label{eq:constraints1}
|\selectivity_\Db(q^*_{a,b})-\selectivity_\Sam(q^*_{a,b})|=\left|\sum_{i=a}^b\varepsilon_{v_i}\right|<\varepsilon\enspace.
\end{equation}
This holds for \emph{all} intervals $[a,b]\subseteq[1,\ell]$.

A similar argument can be made for the queries $q^*_{a,b,\mathrm{rev}}$ with
selection predicate
\[
\mathcal{C} \AND C_f \le v_a \AND C_f \ge v_b\enspace.
\]

These considerations give us the intuition for developing optimization
problems that give deterministic upper and lower bounds to $f(S)$.
In particular, we want to find values for all the $\varepsilon_{v_i}$'s that
satisfy constraints inspired by~\eqref{eq:constraints1} while at the same
time minimizing $f(\tilde S^-)$ (resp.~maximizing $f(\tilde S^+)$), where
$\tilde S^-$ (resp.~$\tilde S^+$) is a set of pairs
$(v_i,c_{v_i})$ where $v_i\in V_{q^*}(C_f)$ and 
\[
c_{v_i} = (\selectivity_\Db(q^*_{v_i})+\varepsilon_{v_i})\eta
\]
with $\eta$ being the product of the sizes in $\Db$ of the tables involved in
$q^*$.

The optimization problem that has to be solved to compute the values
$\varepsilon_{v_i}$ is then the following (for the minimization, and analogously
for the maximization):
\begin{description}
  \item[Constants:] $\varepsilon$, $\eta$,
    $\selectivity_\Sam(q^*_{a,b})$, $\card_\Sam(q^*_{a,b})$,
    $\selectivity_\Sam(q^*_{a,b,\mathrm{rev}})$,
    $\card_\Sam(q^*_{a,b,\mathrm{rev}})$ for each $[a,b]\subseteq[1,\ell]$.
  \item[Variables:] $\varepsilon_{v_i}, \forall v_i\in V_{q^*}(C_f)$ 
  \item[Objective:] minimize the function $f(\tilde S^-)$, where
    $S^-=\{(v_i,(\selectivity_\Sam(q^*_{v_i})+\varepsilon_{v_i})\eta), 1\le i\le
    \ell\}$.
  \item[Constraints:]
    \begin{align*}
    &\max\{\card_\Sam(q^*_{a,b}),
    \lceil(\selectivity_\Sam(q^*_{a,b})-\varepsilon)\eta\rceil\} \le
    \left(\selectivity_\Sam(q^*_{a,b})+\sum_{i=a}^b\varepsilon_{v_i}\right)\eta \le
    \min\{\eta,\lfloor(\selectivity_\Sam(q^*_{a,b})+\varepsilon)\eta\rfloor\}, \forall
    [a,b]\subseteq[1,\ell] \\
    &\max\{\card_\Sam(q^*_{a,b,\mathrm{rev}}),
    \lceil(\selectivity_\Sam(q^*_{a,b,\mathrm{rev}})-\varepsilon)\eta\rceil\} \le
    \left(\selectivity_\Sam(q^*_{a,b,\mathrm{rev}})+\sum_{i=1}^a\varepsilon_{v_i}+\sum_{i=b}^\ell\varepsilon_{v_i}\right)\eta \le
    \min\{1,\lfloor(\selectivity_\Sam(q^*_{a,b,\mathrm{rev}})+\varepsilon)\eta\rfloor\}, \forall [a,b]\subseteq[1,\ell]
    \end{align*}
\end{description}
\question{To be really, really, accurate, one should replace
$\selectivity_\Sam()$ with $\card_\Sam()$ and $\varepsilon_{v_i}$ with an
\emph{integer} variable. As a consequence, the optimization problems become
mixed-integer, and I don't know what the consequences are for the less ``nice''
ones (quasi-linear, quadratic, \ldots). Shall we do that? 
}
%The properties of the $\varepsilon$-approximation are used in the constraints.
%For all $v\in V_{q^*}(C_f)$, the selectivity $\sigma_v$ is within $\varepsilon$ from the
%real selectivity of the query with selection/join predicate as
%in~\eqref{eq:aggregselpred}, and this fact is reflected in the first constraint.
%The same is true for the query $q^*$ and given that its selectivity is the sum
%of the $\sigma_v$, then the absolute of the sum of the possible errors for each
%of the $\sigma_v$ must be bounded by $\varepsilon$, as in the second constraint.
Note that to compute the quantities $\selectivity_\Sam(q^*_{a,b})$,
$\card_\Sam(q^*_{a,b})$, $\selectivity_\Sam(q^*_{a,b,\mathrm{rev}})$, and
$\card_\Sam(q^*_{a,b,\mathrm{rev}})$, we only need to run $q^*$ on $\Sam$, not
each of the $q^*_{a,b}$ and $q^*_{a,b,\mathrm{rev}}$ queries.

The following lemma shows that the optimal solutions to the optimization problems
can be used as deterministic confidence bounds for the real value of the
aggregate query.

\begin{lemma}\label{lem:confbounds}
  For any function $f$, let $S$, $\tilde{S}^+$, and $\tilde{S}^-$ be defined as
  above. Then, $f(\tilde{S}^-)\le f(S)\le f(\tilde{S}^+)$.
\end{lemma}
\begin{proof}
  It follows from the properties of $\Sam$, which is an
  $\varepsilon$-approximation, that $S$ is one of the sets whose elements
  satisfy the constraints in the optimization problems. Hence, $f(S)$ is
  considered as a possible objective value for  each optimization problem, and
  the thesis follows.
\end{proof}

Note that the $\ell(\ell-1)$constraints are \emph{linear functions} of the
variables, which is a highly desirable property to be able to solve the
optimization problem efficiently. The bounds in the constraints are more
sophisticated than those as in~\eqref{eq:constraints1} to ensure that the
optimal solution is ``realistic'', i.e., it is actually possible in practice.
This has the net effect of strengthening the bounds, reducing the feasibility
space, and potentially shortening the width of the computed confidence interval.

Until now, we assumed to have perfect knowledge of $V_{q*}(C_f)$, the set of
values appearing in the column $C_f$ in the output of $q^*$ when the query is
run on $\Db$. When we run $q^*$ on a sample (as we do to approximate the
aggregate query), the output of $q^*$ may only contain a subset
$\tilde{V}_{q^*}(C_f)$ of the values and if we only considered the values in
$\tilde{V}_{q*}(C_f)$, the obtained confidence bounds may not contain the real
output. We therefore have to drop the assumption of knowing $V_{q^*}(C_f)$ and
assume instead to know two values $m$ and $M$, with $m\le M$ such that $m$ is a
lower bound to the values in $V_{q^*}(C_f)$ and $M$ is an upper bound to the
values in $V_{q^*}(C_f)$. We have $V_{q^*}(C_f)\subseteq[m,M]$. The values $m$
and $M$ can be derived either from the selection predicate of $q^*$, if it
involves the column $C_f$, or are usually available, for each column, in the set
of statistics stored by the DMBS for query optimization. Consider the optimization
problems defined on $[m,M]$, that is, we now have a variable $\varepsilon_v$ for
each $v\in[m,M]$. 
\question{Shall we be more formal in defining the problem on
$[m,M]$?} 
We have the following results, which tells us that we can just solve
the optimization problems on $[m,M]$ rather than on $V_{q^*}(C_f)$. 

\begin{lemma}\label{lem:equivoptprobs}
  Every solution to the optimization problem on $V_q$ is a solution to the
  optimization problem on $[m,M]$.
\end{lemma}
\begin{proof}
  By setting the variables $\varepsilon_{v}$ for $v\in([m,M]\setminus V_q)$ to $0$.
\end{proof}

\begin{corollary}
  The solution to the maximization problem on $[m,M]$ is greater or equal to the
  solution to the maximization problem on $V_q$. Analogously for the
  minimization problems.
\end{corollary}

\paragraph{Objective functions for standard aggregates.}
\todo{Complete and improve presentation.}
In this paragraph we
discuss the specific cases of finding confidence for the most common aggregate
functions: sum, average, variance, standard deviation, and median/quantile. We
are particularly interested in the computational complexity of solving the
optimization problems for these aggregate functions.
For sum, there are no particular issues: the objective function is obviously
linear and, given that so are the constraints, the optimization problems can be
solved efficiently (in polynomial time), using any linear program
solver\citemissing.

For average, the function to be optimized is
\begin{equation}\label{eq:average}
\frac{1}{(\selectivity_\Sam(q^*)+\sum_{i=1}^\ell\varepsilon_{v_i})\eta}\sum_{i=1}^\ell
\left(v_i(\selectivity_\Sam(q^*_{v_i})+\varepsilon_{v_i})\eta\right)\enspace.
\end{equation}
This is a \emph{linear-fractional function} and therefore
\emph{quasi-linear}~\citep[Example 3.32]{BoydV04}. It can then be solved
efficiently in polynomial time\citemissing \itodo{see
\url{https://en.wikipedia.org/wiki/Quasiconvex_function}}. Alternatively, one
can use a modified \emph{linear} objective function which gives more
conservative confidence bounds:
\[
\frac{1}{\max\{\card_\Sam(q^*),(\selectivity_\Sam(q^*)-\varepsilon)\eta\}}\sum_{i=1}^\ell
\left(v_i(\selectivity_\Sam(q^*_{v_i})+\varepsilon_{v_i})\eta\right)\enspace.
\]
(the above is for maximization, but an analogous function can be defined for
minimization). As we said, optimal solution to linear programs can be found in
polynomial time.
\todo{I'd like to argue that (in many cases) the optimal solution to the
original problem and the optimal solution to the ``linearized'' problem are the
same. I just have an intuition about this, I'm not sure it is true.}

For variance the objective function is
%\[
%\frac{1}{(\selectivity_\Sam(q^*)+\sum_{i=1}^\ell\varepsilon_{v_i})\eta}\sum_{i=1}^\ell
%\left(\selectivity_\Sam(q^*_{v_i}+\varepsilon_{v_i})\eta\left(v_i - \frac{1}{(\selectivity_\Sam(q^*)+\sum_{i=1}^\ell\varepsilon_{v_i})\eta}\sum_{i=1}^\ell
%v_i(\selectivity_\Sam(q^*_{v_i})+\varepsilon_{v_i})\eta \right)^2\right)\enspace.
%\]
%
\begin{equation}\label{eq:variance}
  \underbrace{
\frac{1}{(\selectivity_\Sam(q^*)+\sum_{i=1}^\ell\varepsilon_{v_i})\eta}\sum_{i=1}^\ell
\left(\selectivity_\Sam(q^*_{v_i}+\varepsilon_{v_i})\eta v_i\right)^2}_{T} -
\underbrace{\left(\frac{1}{(\selectivity_\Sam(q^*)+\sum_{i=1}^\ell\varepsilon_{v_i})\eta}\sum_{i=1}^\ell
v_i(\selectivity_\Sam(q^*_{v_i})+\varepsilon_{v_i})\eta \right)^2}_{R}\enspace.
\end{equation}
Let $T^--(R^-)^2$ be the solution to the optimization problem for minimization
using~\eqref{eq:variance} as objective function. Let now $R^+_{\mathrm{avg}}$ be
the solution to the optimization problem for \emph{maximization} for the
average, i.e., using~\eqref{eq:average} as objective function. Clearly 
\[
T^--(R^+_{\mathrm{avg}})^2\le T^--(R^-)^2\enspace.
\]
Consider now $T$ as function of the variables $\varepsilon_{v_i}$'s. We have
\[
T=\sum_i=1^\ell (g_i)^2/A
\], where $A$ is a \emph{positive affine} function of the variables and $g_i$ is
an non-negative affine function of the variables defined as
\[
g_i=\selectivity_\Sam(q^*_{v_i})v_i + \sum_{j=1}^\ell a^{(i)}_j
\varepsilon_{v_i}
\]
with $a^{(i)}_j=0$ for $j\neq i$, and $a^{i}_i=v_i$, $1\le i,j\le \ell$.

Then each of the functions $(g_i)^2/A$ is
convex, as the composition of affine functions with the quadratic-over-linear
function, which is convex.%~\citep[Exerc.~2.4]{BoydVEx13}. 
Then $T$ is also convex because sum of convex functions. Consider the
minimization problem using $T$ as an objective function, and let $T'^-$ be the
optimal solution to this problem. This optimization problem can be solved in
polynomial time.  \question{One can actually replace $A$ with an upper bound and
have a quadratic optimization problem. Shall we mention that?}
We have
\[
T'^--(R^+_{\mathrm{avg}})^2\le T^--(R^+_{\mathrm{avg}})^2\le T^--(R^-)^2\enspace,
\]
hence we can use the leftmost term of the above expression as a deterministic
\emph{lower} bound for the variance. Consider now the problem of finding an
upper bound to the variance. The situation is more complex. Let $T^+-(R^+)^2$ be
the optimal solution to the \emph{maximization} problem with~\eqref{eq:variance}
as objective function, and let $(R^-_\mathrm{avg})^2$ be the optimal solution to
the \emph{minimization} problem with~\eqref{eq:average} as objective function.
We have
\[
T^+-(R^+)^2\le T^+-(R^-_\mathrm{avg})^2\enspace.
\]
Consider the \emph{maximization} problem with $T$ as objective function and let
$T'^+$ be the optimal solution. Clearly,
\[
T^+-(R^+)^2\le T^+-(R^-_\mathrm{avg})^2\le T'^+-(R^-_\mathrm{avg})^2\enspace.
\]
The issue is though that the maximization problem with $T$ as objective
function is a \emph{convex maximization problem over a polytope}, or equivalent a
\emph{concave minimization problem}~\citep{Benson95}. Concave minimization is
known to be a NP-hard problem even under very mild conditions (quadratic
objective over a hypercube). Nevertheless, the optimal solution must be at one
of the extremal points of the polytope. Thanks to this and other interesting
mathematical properties, there are a number of algorithms try to solve this
problem as efficiently as possible and software solvers that implement these
algorithms. If the solution of a convex maximization problem is deemed to
expensive, one can resort at using the deterministic upper bounds to the
variance presented by~\citet{Haas96}, as they require minimal computation.

For standard deviation, we get the same conclusions as for variance, given that
the former is just the squared root of the latter.
\todo{DBMS's actually have two functions for variance and two functions for
stddev, depending on whether the data in the columns are seen as the entire
population or as a sample of it. The only thing that changes should be the
normalization. We should be able to do both.}

For median/quantile\ldots \todo{Take care of median. I don't really know much about it, and
need to think about the worst case. It seems like you just want to add /
remove elements from one hand to the other.}

\subsection{Discussion}\label{sec:discussion}
\todo{Write.}

\paragraph{Accuracy.} Talk about accuracy of estimation \itodo{Fill.}

\paragraph{Comparison with previous results.} In this paragraph we
compare our confidence bounds with the deterministic confidence bounds presented
by Haas~\cite{Haas97}.  \itodo{Fill.}

